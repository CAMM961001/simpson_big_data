{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Los Simpsons con Big Data\n",
    "\n",
    "## ETL\n",
    "\n",
    "Implementación de la primera parte de la arquitectura. En la práctica se puede **prototipar** este ETL en un cuaderno de Jupyter para experimentar rápido y dejar algo funcional. Al finalizar, lo más adecuado es implementarlo en un script utilizando prácticas de **código de producción**.\n",
    "\n",
    "### Extracción\n",
    "\n",
    "Se desea automatizar la extracción de datos y no hacer un típico _copy and paste_, para lo cual se va a implementar un en un webscrapper que haga un crawl sobre las tablas del HTML.\n",
    "\n",
    "En la aplicación se va a utilizar un _webscrapper_ que ya está implementado en Python dentro de funciones de la librería `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import boto3\n",
    "import awswrangler as wr\n",
    "\n",
    "# Open project config file\n",
    "with open(\"config.yml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file); file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrapp wikipedia url\n",
    "PATH_EPISODES = config['scrapper']['url']\n",
    "simpsons_raw = pd.read_html(PATH_EPISODES)\n",
    "\n",
    "# Print total scrapped objects\n",
    "len(simpsons_raw)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación\n",
    "\n",
    "Haciendo una inspección rápida de la página de Wikipedia, se puede identificar que existe 20 tablas de información relacionada a los episodios. Sin embargo, es necesario hacer alguna modificaciones a cada una antes de incorporarlas a AWS S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job done!\n"
     ]
    }
   ],
   "source": [
    "# List of seasons and movies index\n",
    "seasons = np.arange(\n",
    "    config['scrapper']['seasons'][0]\n",
    "    ,config['scrapper']['seasons'][1]+1)\n",
    "movie = config['scrapper']['movie']\n",
    "\n",
    "# File name counter\n",
    "k = 1\n",
    "for idx in seasons:\n",
    "    # Skip movie from ETL\n",
    "    if idx == movie:\n",
    "        continue\n",
    "    \n",
    "    # Build file name\n",
    "    name = f'season{str(k).zfill(2)}.csv'\n",
    "\n",
    "    # Data transformation\n",
    "    transformed_table = (\n",
    "        # Filter data object\n",
    "        simpsons_raw[idx]\n",
    "        # Rename table columns\n",
    "        .rename(columns=config['scrapper']['names'])\n",
    "        .assign(\n",
    "            # Convert datestr to datetime\n",
    "            air_date = lambda df_: pd.to_datetime(df_.air_date),\n",
    "            # Remove \" from title name\n",
    "            title = lambda df_: df_.title.str.replace('\"', ''),\n",
    "            # Remove wikipedia reference box\n",
    "            viewers = lambda df_: df_.viewers.str.extract(r'([0-9]*.[0-9]*)')\n",
    "        ))\n",
    "\n",
    "    # Save table in project dir\n",
    "    transformed_table.to_csv(os.path.join(config['path']['episodes'], name))\n",
    "    k += 1\n",
    "\n",
    "print('Job done!')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga\n",
    "\n",
    "Como último paso del Pipeline, se hace la carga de datos al _data lake_ para que puedan ser consumidos por algún otro usuario o servicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job done!\n"
     ]
    }
   ],
   "source": [
    "# AWS S3 client and bucket\n",
    "s3 = boto3.client('s3')\n",
    "BUCKET = config['etl']['target_bucket']\n",
    "\n",
    "# Files to upload\n",
    "files = [\n",
    "    x for x in os.listdir(config['path']['episodes'])\n",
    "    if x.__contains__('.csv')\n",
    "    ]\n",
    "files.sort()\n",
    "\n",
    "# Upload tables to cloud\n",
    "for file in files:\n",
    "    # Build file path\n",
    "    file = os.path.join(config['path']['episodes'], file)\n",
    "    \n",
    "    # Upload to AWS S3\n",
    "    s3.upload_file(\n",
    "        Filename=file,\n",
    "        Bucket=BUCKET,\n",
    "        Key=file)\n",
    "\n",
    "print('Job done!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CREATE EXTERNAL TABLE IF NOT EXISTS `simpsons`.`<name>` (\\n    `id_overall`    string,\\n    `id_season`     string,\\n    `title`         string,\\n    `director`      string,\\n    `writer`        string,\\n    `air_date`      string,\\n    `code`          string,\\n    `viewers`       string\\n)\\nCOMMENT \"Catalog of guests in Simpsons Seasons.\"\\nROW FORMAT SERDE \\'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\\'\\nWITH SERDEPROPERTIES (\\'field.delim\\' = \\',\\')\\nSTORED AS INPUTFORMAT \\'org.apache.hadoop.mapred.TextInputFormat\\' OUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\\'\\nLOCATION \\'s3://itam-analytics-dante/simpsons/raw/episodes/\\'\\nTBLPROPERTIES (\\'classification\\' = \\'csv\\', \"skip.header.line.count\"=\"1\");'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = os.path.join(config['path']['queries'], 'create_table.sql')\n",
    "\n",
    "with open(file, 'r') as q_file:\n",
    "    query = q_file.read()\n",
    "q_file.close\n",
    "\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CREATE EXTERNAL TABLE IF NOT EXISTS `simpsons`.`season01` (\\n    `id_overall`    string,\\n    `id_season`     string,\\n    `title`         string,\\n    `director`      string,\\n    `writer`        string,\\n    `air_date`      string,\\n    `code`          string,\\n    `viewers`       string\\n)\\nCOMMENT \"Catalog of guests in Simpsons Seasons.\"\\nROW FORMAT SERDE \\'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\\'\\nWITH SERDEPROPERTIES (\\'field.delim\\' = \\',\\')\\nSTORED AS INPUTFORMAT \\'org.apache.hadoop.mapred.TextInputFormat\\' OUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\\'\\nLOCATION \\'s3://itam-analytics-dante/simpsons/raw/episodes/\\'\\nTBLPROPERTIES (\\'classification\\' = \\'csv\\', \"skip.header.line.count\"=\"1\");'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = query.replace('<name>', 'season01')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CREATE EXTERNAL TABLE IF NOT EXISTS `simpsons`.`<name>` (\\n    `id_overall`    string,\\n    `id_season`     string,\\n    `title`         string,\\n    `director`      string,\\n    `writer`        string,\\n    `air_date`      string,\\n    `code`          string,\\n    `viewers`       string\\n)\\nCOMMENT \"Catalog of guests in Simpsons Seasons.\"\\nROW FORMAT SERDE \\'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\\'\\nWITH SERDEPROPERTIES (\\'field.delim\\' = \\',\\')\\nSTORED AS INPUTFORMAT \\'org.apache.hadoop.mapred.TextInputFormat\\' OUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\\'\\nLOCATION \\'s3://itam-analytics-dante/simpsons/raw/episodes/\\'\\nTBLPROPERTIES (\\'classification\\' = \\'csv\\', \"skip.header.line.count\"=\"1\");'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
